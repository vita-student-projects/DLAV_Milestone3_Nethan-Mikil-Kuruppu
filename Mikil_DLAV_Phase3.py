# -*- coding: utf-8 -*-
"""DLAV-Phase3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1apQZtbgvS2lUxp0khlbBTeq_jfyipL-d

# DLAV Project - Phase 3

In this notebook we will train a very simple planner and test its real-world performance.

The first step is to prepare the data. The code below will download the data from google drive and extract it here for your code to use. Whenever your session gets restarted, remember to re-run this cell to re-download the data.
"""

# Install gdown to handle Google Drive file download
#pip install -q gdown

import gdown
import zipfile

download_url = f"https://drive.google.com/uc?id=1YkGwaxBKNiYL2nq--cB6WMmYGzRmRKVr"
output_zip = "dlav_train.zip"
gdown.download(download_url, output_zip, quiet=False)  # Downloads the file to your drive
with zipfile.ZipFile(output_zip, 'r') as zip_ref:  # Extracts the downloaded zip file
    zip_ref.extractall(".")

download_url = "https://drive.google.com/uc?id=17DREGym_-v23f_qbkMHr7vJstbuTt0if"
output_zip = "dlav_val_real.zip"
gdown.download(download_url, output_zip, quiet=False)
with zipfile.ZipFile(output_zip, 'r') as zip_ref:
    zip_ref.extractall(".")

download_url = "https://drive.google.com/uc?id=1_l6cui0pCJ_caixN0uTkkUOfu6ICO8u5"
output_zip = "test_public_real.zip"
gdown.download(download_url, output_zip, quiet=False)
with zipfile.ZipFile(output_zip, 'r') as zip_ref:
    zip_ref.extractall(".")

"""Now lets load the data and visualize how the real data looks like!"""

import pickle
import matplotlib.pyplot as plt
import numpy as np
import random
import os
k = 4
# load the data
data = []
test_files = os.listdir("val_real")

for i in range(k):
    with open(f"val_real/{test_files[i]}", "rb") as f:
        data.append(pickle.load(f))

# plot the camera view of current step for the k examples
fig, axis = plt.subplots(1, k, figsize=(4*k, 4))
for i in range(k):
    axis[i].imshow(data[i]["camera"])
    axis[i].axis("off")
plt.show()

# plot the past and future trajectory of the vehicle
fig, axis = plt.subplots(1, k, figsize=(4*k, 4))
for i in range(k):
    axis[i].plot(data[i]["sdc_history_feature"][:, 0], data[i]["sdc_history_feature"][:, 1], "o-", color="gold", label="Past")
    axis[i].plot(data[i]["sdc_future_feature"][:, 0], data[i]["sdc_future_feature"][:, 1], "o-", color="green", label="Future")
    axis[i].legend()
    axis[i].axis("equal")
plt.show()



"""Now let's train the model with synthetic data and test it with the real-world data."""

import torch
from torch.utils.data import Dataset
import pickle

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import random
import math
import numpy as np

class TrajectoryAwareLoss(nn.Module):
    def __init__(self, coord_weight=1.0, velocity_weight=0.2):
        super().__init__()
        self.coord_weight = coord_weight
        self.velocity_weight = velocity_weight
        
    def forward(self, pred, target):
        coord_loss = F.mse_loss(pred[..., :2], target[..., :2])
        pred_vel = pred[:, 1:, :2] - pred[:, :-1, :2]
        target_vel = target[:, 1:, :2] - target[:, :-1, :2]
        velocity_loss = F.mse_loss(pred_vel, target_vel)
        return self.coord_weight * coord_loss + self.velocity_weight * velocity_loss

class DrivingDataAugmenter:
    def __init__(self, image_aug_prob=0.6, trajectory_aug_prob=0.4, seed=None):
        self.image_aug_prob = image_aug_prob
        self.trajectory_aug_prob = trajectory_aug_prob
        
        if seed is not None:
            torch.manual_seed(seed)
            random.seed(seed)
            np.random.seed(seed)
        
        self.image_transforms = transforms.Compose([
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),
            transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.2),
        ])
    
    def augment_image(self, image):
        if random.random() < self.image_aug_prob:
            image = self.image_transforms(image)
            
            if random.random() < 0.5:
                noise_level = random.uniform(0.01, 0.03)
                image = image + torch.randn_like(image) * noise_level
                image = torch.clamp(image, 0, 255)
            
            if random.random() < 0.2:
                h, w = image.shape[1], image.shape[2]
                shadow_size = min(h, w) // 4
                shadow_x = random.randint(0, max(1, w - shadow_size))
                shadow_y = random.randint(0, max(1, h - shadow_size))
                darkness = random.uniform(0.7, 0.9)
                
                mask = torch.ones_like(image)
                mask[:, shadow_y:shadow_y+shadow_size, shadow_x:shadow_x+shadow_size] = darkness
                image = image * mask
            
            if random.random() < 0.15:
                haze_intensity = random.uniform(0.05, 0.15)
                haze = torch.ones_like(image) * 255 * haze_intensity
                image = image * (1 - haze_intensity) + haze
                image = torch.clamp(image, 0, 255)
        
        return image
    
    def augment_trajectory(self, trajectory):
        if random.random() < self.trajectory_aug_prob:
            noise_intensity = random.uniform(0.01, 0.03)
            base_noise = torch.randn_like(trajectory) * noise_intensity
            
            time_weight = torch.linspace(1.2, 0.3, trajectory.size(0)).unsqueeze(1).to(trajectory.device)
            weighted_noise = base_noise * time_weight
            
            trajectory = trajectory + weighted_noise
            
            if random.random() < 0.2:
                angle = random.uniform(-0.02, 0.02)
                cos_val, sin_val = math.cos(angle), math.sin(angle)
                rotation_matrix = torch.tensor([
                    [cos_val, -sin_val], 
                    [sin_val, cos_val]
                ]).to(trajectory.device)
                
                xy_coords = trajectory[:, :2]
                rotated_xy = torch.matmul(xy_coords, rotation_matrix.t())
                trajectory[:, :2] = rotated_xy
            
            if random.random() < 0.2:
                scale_factor = random.uniform(0.98, 1.02)
                trajectory[:, :2] = trajectory[:, :2] * scale_factor
        
        return trajectory

class DrivingDataset(Dataset):
    def __init__(self, file_list, test=False, use_augmentation=False, aug_seed=42):
        self.samples = file_list
        self.test = test
        self.use_augmentation = use_augmentation
        self.augmenter = DrivingDataAugmenter(seed=aug_seed) if use_augmentation else None

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        # Load pickle file
        with open(self.samples[idx], 'rb') as f:
            data = pickle.load(f)

        # Convert numpy arrays to tensors
        camera = torch.FloatTensor(data['camera']).permute(2, 0, 1)
        history = torch.FloatTensor(data['sdc_history_feature'])
        
        # Apply augmentations only during training (not testing)
        if self.use_augmentation and not self.test:
            torch.manual_seed(42 + idx)
            random.seed(42 + idx)
            
            camera = self.augmenter.augment_image(camera)
            history = self.augmenter.augment_trajectory(history)
        
        if not self.test:
            future = torch.FloatTensor(data['sdc_future_feature'])
            return {
                'camera': camera,
                'history': history,
                'future': future
            }
        else:
            return {
                'camera': camera,
                'history': history
            }

class Logger:
    def __init__(self):
        # Placeholder for potential future configs (e.g., log_dir, wandb_enabled, etc.)
        pass

    def log(self, step=None, **metrics):
        """
        Logs the given metrics.

        Args:
            step (int, optional): The current step or epoch. Useful for tracking.
            **metrics: Arbitrary keyword arguments representing metric names and values.
        """
        prefix = f"[Step {step}] " if step is not None else ""
        metric_str = " | ".join(f"{k}: {v}" for k, v in metrics.items())
        # print(prefix + metric_str)

"""Note: Replace with your own planner below"""

import torch
import torch.nn as nn

import torch
import torch.nn as nn

class DrivingPlanner(nn.Module):
    def __init__(self):
        super().__init__()

        # CNN for processing camera images
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )

        # MLP for processing history features
        self.history_encoder = nn.Sequential(
            nn.Linear(21 * 3, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # Decoder for predicting future trajectory
        self.decoder = nn.Sequential(
            nn.Linear(512, 512),  # 256 (CNN) + 256 (history)
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 60 * 3)  # Predict 60 timesteps of [x, y, heading]
        )

    def forward(self, camera, history):
        # Process camera images
        visual_features = self.cnn(camera)

        # Process history
        history_flat = history.reshape(history.size(0), -1)
        history_features = self.history_encoder(history_flat)

        # Combine features
        combined = torch.cat([visual_features, history_features], dim=1)

        # Predict future trajectory
        future = self.decoder(combined)
        future = future.reshape(-1, 60, 3)  # Reshape to (batch_size, timesteps, features)

        return future

def train(model, train_loader, val_loader, optimizer, scheduler, logger, num_epochs=60):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = TrajectoryAwareLoss()
    best_ade = float('inf')

    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0
        for idx, batch in enumerate(train_loader):
            camera = batch['camera'].to(device)
            history = batch['history'].to(device)
            future = batch['future'].to(device)

            optimizer.zero_grad()
            pred_future = model(camera, history)
            loss = criterion(pred_future, future)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            if idx % 10 == 0:
                logger.log(step=epoch * len(train_loader) + idx, loss=loss.item())
            train_loss += loss.item()

        scheduler.step()

        # Validation
        model.eval()
        val_loss, ade_all, fde_all = 0, [], []
        with torch.no_grad():
            for batch in val_loader:
                camera = batch['camera'].to(device)
                history = batch['history'].to(device)
                future = batch['future'].to(device)

                pred_future = model(camera, history)
                loss = F.mse_loss(pred_future[..., :2], future[..., :2])
                ADE = torch.norm(pred_future[:, :, :2] - future[:, :, :2], p=2, dim=-1).mean()
                FDE = torch.norm(pred_future[:, -1, :2] - future[:, -1, :2], p=2, dim=-1).mean()
                ade_all.append(ADE.item())
                fde_all.append(FDE.item())
                val_loss += loss.item()

        avg_ade = np.mean(ade_all)
        if avg_ade < best_ade:
            best_ade = avg_ade
            torch.save(model.state_dict(), 'best_model.pth')

        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f} | ADE: {np.mean(ade_all):.4f} | FDE: {np.mean(fde_all):.4f}')

    if os.path.exists('best_model.pth'):
        model.load_state_dict(torch.load('best_model.pth'))

import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR
import os

"""We see a hugh sim-to-real gap here! Although the planner works well with synthetic sensors, it cannot generalize to the real wolrd scenario.

How to generalize your planner to the real world? Let's expore a simple trick: data mixing. This time we mix part of the real data with the synthetic data to train the model.
"""

import torch.optim as optim
from torch.utils.data import DataLoader
import os

train_data_dir = "train"
real_data_dir = "val_real"

train_files = [os.path.join(train_data_dir, f) for f in os.listdir(train_data_dir) if f.endswith('.pkl')]
test_files = [os.path.join(real_data_dir, f) for f in os.listdir(real_data_dir) if f.endswith('.pkl')]

train_files_mixed = train_files + test_files[:500]
test_files = test_files[500:]

# Enable augmentation for training data
train_dataset = DrivingDataset(train_files_mixed, use_augmentation=True)  # <- Enable augmentation
val_dataset = DrivingDataset(test_files)  # No augmentation for validation

train_loader = DataLoader(train_dataset, batch_size=32, num_workers=2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)

model = DrivingPlanner()

optimizer = optim.AdamW(model.parameters(), lr=8e-4, weight_decay=0.01)
scheduler = CosineAnnealingLR(optimizer, T_max=60, eta_min=1e-6)

logger = Logger()

train(model, train_loader, val_loader, optimizer, scheduler, logger, num_epochs=60)

"""Finally, generate your submission."""

import pandas as pd
test_data_dir = "test_public_real"
test_files = [os.path.join(test_data_dir, fn) for fn in sorted([f for f in os.listdir(test_data_dir) if f.endswith(".pkl")], key=lambda fn: int(os.path.splitext(fn)[0]))]

test_dataset = DrivingDataset(test_files, test=True)
test_loader = DataLoader(test_dataset, batch_size=250, num_workers=2)
model.eval()
all_plans = []
device = 'cuda'
with torch.no_grad():
    for batch in test_loader:
        camera = batch['camera'].to(device)
        history = batch['history'].to(device)

        pred_future = model(camera, history)
        all_plans.append(pred_future.cpu().numpy()[..., :2])
all_plans = np.concatenate(all_plans, axis=0)

# Now save the plans as a csv file
pred_xy = all_plans[..., :2]  # shape: (total_samples, T, 2)

# Flatten to (total_samples, T*2)
total_samples, T, D = pred_xy.shape
pred_xy_flat = pred_xy.reshape(total_samples, T * D)

# Build a DataFrame with an ID column
ids = np.arange(total_samples)
df_xy = pd.DataFrame(pred_xy_flat)
df_xy.insert(0, "id", ids)

# Column names: id, x_1, y_1, x_2, y_2, ..., x_T, y_T
new_col_names = ["id"]
for t in range(1, T + 1):
    new_col_names.append(f"x_{t}")
    new_col_names.append(f"y_{t}")
df_xy.columns = new_col_names

# Save to CSV
df_xy.to_csv("submission_phase3.csv", index=False)

print(f"Shape of df_xy: {df_xy.shape}")